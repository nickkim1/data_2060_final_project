{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54009b2-e747-4402-91c0-18562e915c6c",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification: One-vs-All and All-Pairs**\n",
    "Logistic regression is one type of classification algorithm that determines, for a given set of feature values, the most likely label from a given set of labels. Binary logistic regression refers to the case that there are exactly two labels to choose from while multiclass regression is the more general case where there could be any given number of labels. The type of multiclass classification we will be implementing is built based on binary logistic regression, so we first discuss the binary case. \n",
    "\n",
    "\n",
    "### **Binary Logistic Regression**\n",
    "\n",
    "**Representation:** Suppose our data points have $d$ numeric features and a labeled of either $1$ or $0$. Rather than returning a label of $1$ or $0$, binary logistic regression returns the probability of a data point belonging to class $1$. The predicted class is then $1$ if the probability of beloning to $1$ is greater than $50\\%$ and is $0$ if the probability of belonging to $1$ is less than $50\\%$. To find the probability of a data point beloning to class $1$, the following calculation is performed, \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}}$$\n",
    "where $x$ is the set of features and $w$ is a weights vector. \n",
    "\n",
    "Then if $\\langle w, x \\rangle > 0$, we will have $e^{-\\langle w, x \\rangle} < 1$, so our probability of being class $1$ is \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}} > \\frac{1}{1+1} = \\frac{1}{2}.$$\n",
    "Alternatively, if $\\langle w, x \\rangle < 0$, we will have $e^{-\\langle w, x \\rangle} > 1$, so our probability of being class $1$ is \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}} < \\frac{1}{1+1} = \\frac{1}{2}.$$\n",
    "In summary, if $\\langle w, x \\rangle > 0$, then the predicted class will be $1$. If $\\langle w, x \\rangle < 0$, then the predicted class will be $0$. This is then how our decision boundary is defined; the decision boundary is all vectors $x$ such that $\\langle w, x \\rangle = 0$. All vectors that lie on the side to which $w$ is pointing will be designated $1$ while all those that lie on the side away from which $w$ is pointing will be designated $0$.\n",
    "\n",
    "**Loss:** Training this algorithm means determining the optimal weight vector $w$. Optimal here means the weight vector that minimizes error, so we must both quanitfy the error and find some way to minimize it. We quantify the error with Binary Cross Entropy Loss: \n",
    "\n",
    "$$L_s(h_w) = -\\frac{1}{m}\\sum_{i=1}^m(y_i\\log h_w(x_i)+(1-y_i)\\log(1-h_w(x_i)))$$\n",
    "\n",
    "The next step is to find weights to minimize this. We will do that through a process called Stochastic Gradient Descent.\n",
    "\n",
    "**Optimizer - Stochastic Gradient Descent:** For a given set of points with features $x_i$ and labels $y_i$, we can view the error as a function of the weight vector $w$. We want to find the $w$ that minimizes our function, and from calculus we know that the gradient of the function always points away from the minimum. We can therefore approximately locate the minimum by moving in the opposite direction of the gradient in a process known as gradient descent. Starting from some initial guess, each subsequent value is achieved by moving a set distance along the function in the opposite direction of the gradient. This set distance is the step size, and the choice of step size determines whether or not gradient descent can converge. If the step size is too small, the minimum may not have been reached after thousands of timesteps while if the step size is too big, a single step might overshoot the minimum and leave the algorithm jumping back and forth to each side of the minimum but never actually getting closer. Classical gradient descent involves using all of the training data to calculate the gradient of the loss explicitly at every step; this results in a very direct but very slow path to the minimum. Stochastic gradient descent (SGD) speeds up the process by only using a smaller subset of the data to approximate the gradient of the loss function. In SGD, the data is shuffled then split into evenly sized batches. One batch is used to calculate the gradient, then a step is taken, then the next batch is used and another step is taken, repeating until all of the batches have been used. If the algorithm still has not converged after going through all of the batches, the data is shuffled and then split into new batches and the procedure is repeated. Since we do not know a priori what the minimum is, we cannot determine convergence based on distance to the minimum. Convergence must therefore be approximated by checking how much the weights are changing with every step or checking how much the loss is changing.\n",
    "\n",
    "Using the representation, loss, and optimizer described so far, we have a fully defined method for binary classification. We now build upon this to create a multiclass classification model. \n",
    "\n",
    "\n",
    "### **One-vs-All**\n",
    "\n",
    "One-vs-all is a method where a binary classification algorithm can be used for multiclass classification by splitting the multiclass problem into several binary problems. Assume that we have $k$ classes. To apply the one-vs-all method, we train $k$ binary logistic regression models, each of which predicts the probability of being in one of the specific $k$ classes. Once this has been done, the final predicted class of each data point is determined by labeling it with the class that it had the highest probability of belonging to. Implementing this algorithm would look something like the steps outlined below. \n",
    "\n",
    "$\\text{inputs:}$\n",
    "\n",
    "$\\text{S = the training set comprised of $m$ labeled sets of features $(\\vec{x}_i, y_i)$}$\n",
    "\n",
    "$A = \\text{ the binary classification algorithm in use, ex. binary logistic regression}$\n",
    "\n",
    "$\\text{For each } k \\in \\text{ the label set } \\mathcal{Y}:$\n",
    "\n",
    "$\\quad \\text{Define  $S_k$ to be the set of all examples but where the label is 1 if the example has label $k$ and 0 otherwise}$\n",
    "\n",
    "$\\quad \\text{Define } h_k = A(S_k) \\text{ as a binary predictor that predicts 1 if } x \\in S_k$\n",
    "\n",
    "$\\text{Output:}$\n",
    "\n",
    "$\\quad \\text{Define the multiclass predictor by }h(\\vec{x}) = \\text{argmax}_{k \\in \\mathcal{Y}} h_i(\\vec{x})$\n",
    "\n",
    "In summary, the multiclass predictor is composed of one binary predictor for each class that simply predicts whether or not the data belongs to that class. These prections are then gathered and the final label is the class that $\\vec{x}$ had the highest probability of being in. \n",
    "\n",
    "### **All-Pairs**\n",
    "\n",
    "The other multiclass classifier we will be examining is the all-pairs or one-vs-one classifier. Like the one-vs-all algorithm, the all-pairs algorithm builds a multiclass predictor out of several binary predictors. For this predictor, for each pair $i,j$ of classes, construct a subset $S_{i,j}$ of the training data $S$ that contains only the examples that are in class $i$ or $j$. Then for each subset $S_{i,j}$, set the label of $\\vec{x}$ to $1$ if $\\vec{x}$ is in class $i$ and 0 if $x$ is in class $j$. A binary classifier $h_{i,j}$ is defined for each $S_{i,j}$. A binary classifier $h_{i,j}$ is defined for each $S_{i,j}$. To get the final prediction for the class of $\\vec{x}$, $\\vec{x}$ is fed into each of the binary classifiers $h_{i,j}$. The predicted class of $\\vec{x}$ is chosen to be the class that was predicted most frequently across all of the binary classifiers. Implementing this algorithm would look something like the steps outlined below. \n",
    "\n",
    "$\\text{inputs:}$\n",
    "\n",
    "$S = \\text{ the training set comprised of $m$ labeled sets of features}(\\vec{x}_i, y_i)$\n",
    "\n",
    "$A = \\text{ the binary classification algorithm in use, ex. binary logistic regression}$\n",
    "\n",
    "$\\text{For each } i,j \\in \\text{ the label set } \\mathcal{Y} \\text{ with } i < j:$\n",
    "\n",
    "$\\quad \\text{Construct } S_{i,j} \\text{ by adding each $\\vec{x}_k$ such that } y_k \\in \\{i,j\\}$\n",
    "\n",
    "$\\quad \\text{Define } h_{i,j} = A(S_{i,j}) \\text{ as a binary predictor that predicts either $i$ (1) or $j$ (0) for each input }$\n",
    "\n",
    "$\\text{Output:}$\n",
    "\n",
    "$\\quad \\text{Define the multiclass predictor by }h(\\vec{x}) = \\text{argmax}_{i \\in \\mathcal{Y}} \\left( \\sum_{j \\in \\mathcal{Y}} \\text{sign}(j-i)H_{i,j}(\\vec{x}) \\right)$\n",
    "\n",
    "### **Advantages and Disadvantages**\n",
    "\n",
    "One big advantage of each of these methods is that they are fairly simple, making them easy to understand and to implement. Each is constructed based on a binary classifier, so implementation requires simply repeatedly calling an existing algorithm. The largest downside to each of these methods is that the binary learner that is implemented does not know that its output hypotheses will be combined to construct a multiclass predictor. This can lead to issues because the binary learner will be trying to minimize the error of each individual hypothesis rather than the overall error. \n",
    "\n",
    "The One-vs-All method trains only one binary learner for each class, but this binary learner must be trained on all data points. The All-Pairs method requires a binary learner to be trained for each pair of classes, but this binary learner is only trained on the subset of data points which are labeled with one of the classes in the pair. This means that One-vs-all may be preferable in cases with a high number of classes while All-pairs may be preferred if there are fewer classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b63416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0476a",
   "metadata": {},
   "source": [
    "### Model\n",
    "Binary logistic regression model + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae0feee-d2c6-4d89-8d54-8b41df0542b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class RegularizedLogisticRegression(object):\n",
    "    '''\n",
    "    Implement regularized logistic regression for binary classification.\n",
    "    The weight vector w should be learned by minimizing the regularized loss\n",
    "    L(h, (x,y)) = log(1 + exp(-y <w, x>)) + lambda |w|_2^2. In other words, the objective\n",
    "    function that we are trying to minimize is the log loss for binary logistic regression \n",
    "    plus Tikhonov regularization with a coefficient of lambda.\n",
    "    '''\n",
    "    def __init__(self, batch_size = 32, learning_rate = 0.01, num_epochs = 100, one_vs_all = True, all_pairs = False):\n",
    "        self.learningRate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.lmbda = 0.001\n",
    "        self.one_vs_all = one_vs_all\n",
    "        self.all_pairs = all_pairs\n",
    "\n",
    "        # Make the losses a dictionary. Mapping from epoch number -> Loss. At the end\n",
    "        # average over the loss per-epoch, which should consist of averaging the losses over\n",
    "        # all of the binary classifiers run in that epoch.\n",
    "        self.losses = defaultdict(list)\n",
    "\n",
    "    def one_vs_all_train(self, X, Y):\n",
    "        '''\n",
    "        One-vs-all algorithm for multi-class classification.\n",
    "         @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return: nothing\n",
    "        '''\n",
    "        # First, relabel the labels in the dataset. Denote the # of labels as k.\n",
    "        uq_labels = np.unique(Y)\n",
    "        _, d = X.shape\n",
    "\n",
    "        # Our weights matrix should be of size (num_classes, num_features)\n",
    "        # and we argmax over all the respective logits\n",
    "        self.weights = [np.zeros((1, d)) for _ in range(len(uq_labels))]\n",
    "        \n",
    "        # This will be a (k, m) array. Later, we will take the argmax. over\n",
    "        # axis=0 to get the class w/ largest probability for that i'th point.\n",
    "        for i, label in enumerate(uq_labels):\n",
    "\n",
    "            # Train a binary classifier for each class, collate logits\n",
    "            binarized_Y = (Y == label).astype(int)\n",
    "            self.train(X, binarized_Y, self.weights[i])\n",
    "\n",
    "        \n",
    "    def all_pairs_train(self, X, Y):\n",
    "        '''\n",
    "        All-pairs algorithm for multi-class classification.\n",
    "         @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return: nothing\n",
    "        '''\n",
    "        # First, relabel the labels in the dataset. Denote the # of labels as k.\n",
    "        _, d = X.shape\n",
    "        uq_labels = np.unique(Y)\n",
    "        num_labels = len(uq_labels)\n",
    "\n",
    "        # Create a combinatorial weight matrix storing weights for each classifier pair\n",
    "        # Only the upper triangle of these weights will be used, so a bit space inefficient but it's convenient\n",
    "        self.weights = [[np.zeros((1, d)) for _ in range(num_labels)] for _ in range(num_labels)]\n",
    "\n",
    "        # Loop over all of the label combinations\n",
    "        # i'th dimension: the actual class you want to predict probabilities for.\n",
    "        # j'th dimension: all other classes you're predicting against.\n",
    "        # Summing up over the j'th dimension gets the cumulative probability for the i'th class.\n",
    "        for i in range(num_labels):\n",
    "            for j in range(num_labels):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                mask = (Y == i) | (Y == j)\n",
    "                X_ij = X[mask]\n",
    "                Y_ij = (Y[mask] == i).astype(int)\n",
    "\n",
    "                # Train binary classifier, collate logits\n",
    "                X_ij, Y_ij = np.array(X_ij), np.array(Y_ij)\n",
    "                self.train(X_ij, Y_ij, self.weights[i][j])\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        _, d = X.shape\n",
    "        if self.one_vs_all:\n",
    "            # Do some reshaping for shape compatibility\n",
    "            weights = np.array(self.weights).reshape(len(self.weights), d)\n",
    "            # Apply sigmoid function to get probabilities \n",
    "            weights = np.apply_along_axis(sigmoid_function, 1, weights)\n",
    "            # Argmax over all probabilities per each class\n",
    "            return np.argmax(X @ weights.T, axis=1)\n",
    "        elif self.all_pairs:\n",
    "            num_labels = len(self.weights)\n",
    "            # This will end up being a 3d array, of size: (num_labels x num_labels x m)\n",
    "            #                                                   i           j       k\n",
    "            # We will sum up over the j'th dimension, then argmax over the i'th dimension.\n",
    "            # This collapses both the i'th and the j'th dimension, leaving us with a (1, m) result.\n",
    "            probs_ij = [[] for _ in range(num_labels)]\n",
    "            probs_i = [0 for _ in range(num_labels)]\n",
    "            for i in range(num_labels):\n",
    "                for j in range(num_labels):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "\n",
    "                    probs_ij[i].append(sigmoid_function(X @ self.weights[i][j].T).T)\n",
    "                # Sum up over the j'th dimension. Since each j'th element is a 1d array of size (1, m)\n",
    "                # the j'th dimension is axis=0, and we want to sum up over it for all m examples.\n",
    "                probs_i[i] = np.sum(np.array(probs_ij[i]), axis=0)\n",
    "            # Then, once we're completely done with prediction, we can argmax over axis=0\n",
    "            # on probs_i so that we get a (1, m) array as expected.\n",
    "            return np.argmax(probs_i, axis=0)\n",
    "\n",
    "    def train(self, X, Y, weights):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        num_examples, _ = X.shape\n",
    "\n",
    "        # Explicitly add a bias term to the input\n",
    "        np.hstack([X, np.ones((num_examples, 1))])\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Shuffle the dataset\n",
    "            shuffled_indices = np.random.permutation(num_examples)\n",
    "\n",
    "            # X: (m,d) Y: (m,1) (just add an additional dim to make later matrix operations easier)\n",
    "            X, Y = X[shuffled_indices], Y[shuffled_indices].reshape(len(Y), 1)\n",
    "\n",
    "            # Iterate over ALL batches even if not evenly divisible.\n",
    "            for batch_no in range(0, num_examples, self.batch_size):\n",
    "                X_batch = X[batch_no : batch_no + self.batch_size]\n",
    "                Y_batch = Y[batch_no : batch_no + self.batch_size]\n",
    "\n",
    "                # Compute logits: (m,). Flatten from (m, 1) though since you need to broadcast w/ Y_batch which is (m,)\n",
    "                Z_batch = sigmoid_function(X_batch @ weights.T)\n",
    "\n",
    "                # Vectorized loss gradient matrix computation: (m, d).T x (m, 1) -> (d, 1). To be able to add self.weights, transpose.\n",
    "                gL_w = X_batch.T @ (Z_batch - Y_batch) / len(X_batch) + (2 * self.lmbda * weights.T)\n",
    "\n",
    "                # Note you have to transpose 'back' here \n",
    "                weights -= self.learningRate * gL_w.T\n",
    "            \n",
    "            # Compute the binary loss for this epoch, accumulate it into the losses dict\n",
    "            self.losses[epoch].append(self.loss(X, Y, weights))\n",
    "\n",
    "    \n",
    "    def loss(self, X, y, weights):\n",
    "        '''\n",
    "        Cmputes binary cross entropy loss given preds, labels.\n",
    "        @params:\n",
    "            X: Input data\n",
    "            y: Labels for the predictions\n",
    "        @return:\n",
    "            The loss.\n",
    "        '''\n",
    "        preds = sigmoid_function(X @ weights.T)\n",
    "        return -(1 / len(preds)) * np.sum((y * np.log(preds) + (1 - y) * np.log(1 - preds)), axis=0)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        return np.mean(self.predict(X) == Y)\n",
    "\n",
    "    def runTrainTestValSplit(self, lambda_list, X_train, Y_train, X_val, Y_val):\n",
    "        '''\n",
    "        Given the training and validation data, fit the model with training data and test it with\n",
    "        respect to each lambda. Record the training error and validation error, which are equivalent \n",
    "        to (1 - accuracy).\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X_train: a 2D Numpy array for trainig where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_train: a 1D Numpy array for training containing the corresponding labels for each example\n",
    "            X_val: a 2D Numpy array for validation where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_val: a 1D Numpy array for validation containing the corresponding labels for each example\n",
    "        @returns:\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "        '''\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        # Train model and calculate train and validation errors here for each lambda\n",
    "\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "\n",
    "            # Train, validate, log errors\n",
    "            if self.one_vs_all:\n",
    "                self.one_vs_all_train(X_train, Y_train)\n",
    "            elif self.all_pairs:\n",
    "                self.all_pairs_train(X_train, Y_train)\n",
    "            train_errors.append(1 - self.accuracy(X_train, Y_train))\n",
    "            val_errors.append(1 - self.accuracy(X_val, Y_val))\n",
    "\n",
    "        return train_errors, val_errors\n",
    "\n",
    "    def _kFoldSplitIndices(self, dataset, k):\n",
    "        '''\n",
    "        Helper function for k-fold cross validation. Evenly split the indices of a\n",
    "        dataset into k groups.\n",
    "        For example, indices = [0, 1, 2, 3] with k = 2 may have an output\n",
    "        indices_split = [[1, 3], [2, 0]].\n",
    "        \n",
    "        Please don't change this.\n",
    "        @params:\n",
    "            dataset: a Numpy array where each row contains an example\n",
    "            k: an integer, which is the number of folds\n",
    "        @return:\n",
    "            indices_split: a list containing k groups of indices\n",
    "        '''\n",
    "        num_data = dataset.shape[0]\n",
    "        fold_size = int(num_data / k)\n",
    "        indices = np.arange(num_data)\n",
    "        np.random.shuffle(indices)\n",
    "        indices_split = np.split(indices[:fold_size*k], k)\n",
    "        return indices_split\n",
    "\n",
    "    def runKFold(self, lambda_list, X, Y, k = 3):\n",
    "        '''\n",
    "        Run k-fold cross validation on X and Y with respect to each lambda. Return all k-fold\n",
    "        errors.\n",
    "        \n",
    "        Each run of k-fold involves k iterations. For an arbitrary iteration i, the i-th fold is\n",
    "        used as testing data while the rest k-1 folds are combined as one set of training data. The k results are\n",
    "        averaged as the cross validation error.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "            k: an integer, which is the number of folds, k is 3 by default\n",
    "        @return:\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        '''\n",
    "        k_fold_errors = []\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "            # Call _kFoldSplitIndices to split indices into k groups randomly\n",
    "            indices = self._kFoldSplitIndices(X, k)\n",
    "\n",
    "            # For each iteration i = 1...k, train the model using lmbda\n",
    "            # on kâˆ’1 folds of data. Then test with the i-th fold.\n",
    "            total_fold_error = 0\n",
    "            for i in range(k):\n",
    "\n",
    "                val_indices = indices[i]\n",
    "                X_val, Y_val = X[val_indices], Y[val_indices]\n",
    "\n",
    "                # You just want to hstack the arrays in this list, since you basically have a bunch\n",
    "                # of arrays containing valid train indices and you want to combine them all now\n",
    "                train_indices = np.hstack(indices[:i] + indices[i+1:])\n",
    "\n",
    "                X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "\n",
    "                if self.one_vs_all:\n",
    "                    self.one_vs_all_train(X_train, Y_train)\n",
    "                elif self.all_pairs:\n",
    "                    self.all_pairs_train(X_train, Y_train)\n",
    "                total_fold_error += 1 - self.accuracy(X_val, Y_val)\n",
    "\n",
    "            # Calculate and record the cross validation error by averaging total errors\n",
    "            k_fold_errors.append(total_fold_error / k)\n",
    "\n",
    "        return k_fold_errors\n",
    "\n",
    "    def plotError(self, lambda_list, train_errors, val_errors, k_fold_errors):\n",
    "        '''\n",
    "        Produce a plot of the cost function on the training and validation sets, and the\n",
    "        cost function of k-fold with respect to the regularization parameter lambda. Use this plot\n",
    "        to determine a valid lambda.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.semilogx(lambda_list, train_errors, label = 'training error')\n",
    "        plt.semilogx(lambda_list, val_errors, label = 'validation error')\n",
    "        plt.semilogx(lambda_list, k_fold_errors, label = 'k-fold error')\n",
    "        plt.xlabel('lambda')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37571d1b",
   "metadata": {},
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5808902",
   "metadata": {},
   "source": [
    "These are unit tests for methods in our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698e225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "one_vs_all_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=True, all_pairs=False)\n",
    "all_pairs_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=False, all_pairs=True)\n",
    "\n",
    "# ==== TEST BINARY ======\n",
    "X1 = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "Y1 = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "X1_test = np.array([[0, 0, 1], [-5, 3, 1], [9, 0, 1], [1, 0, 1], [6, -7, 1]])\n",
    "Y1_test = np.array([0, 0, 1, 0, 1])\n",
    "X_multi_class = np.array([[1, 0, 1], [0, 1, 1], [2, 1, 1], [3, 0, 1], [0, 3, 1], [4, 0, 1]])\n",
    "Y_multi_class = np.array([0, 1, 2, 2, 1, 0])\n",
    "\n",
    "# train one vs. all model\n",
    "one_vs_all_model.one_vs_all_train(X1, Y1)\n",
    "w1 = one_vs_all_model.weights\n",
    "# ** Note the slightly 'weird' shape of our weights matrix is just for implementation ease-of-use\n",
    "assert np.array(w1).shape == (2, 1, 3)\n",
    "\n",
    "# check all pairs model\n",
    "all_pairs_model.all_pairs_train(X1, Y1)\n",
    "w_all_pairs = all_pairs_model.weights\n",
    "# ** Note the slightly 'weird' shape of our weights matrix is just for implementation ease-of-use\n",
    "assert np.array(w_all_pairs).shape == (2, 2, 1, 3)\n",
    "\n",
    "# test model prediction for both\n",
    "one_vs_all_pred = one_vs_all_model.predict(X1_test)\n",
    "assert isinstance(one_vs_all_pred, np.ndarray)\n",
    "assert one_vs_all_pred.shape == (5,)\n",
    "assert (one_vs_all_pred == np.array([0, 0, 1, 1, 1])).all()\n",
    "\n",
    "# ==== TEST MULTICLASS ====\n",
    "one_vs_all_mc_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=True, all_pairs=False)\n",
    "one_vs_all_mc_model.one_vs_all_train(X_multi_class, Y_multi_class == 0)\n",
    "one_vs_all_mc_preds = one_vs_all_mc_model.predict(X_multi_class)\n",
    "assert isinstance (one_vs_all_mc_preds, np.ndarray)\n",
    "assert one_vs_all_mc_preds.shape == (6,)\n",
    "assert set(one_vs_all_mc_preds).issubset({0, 1, 2})\n",
    "\n",
    "all_pairs_model.all_pairs_train(X_multi_class, Y_multi_class == 0)\n",
    "all_pairs_preds = all_pairs_model.predict(X_multi_class)\n",
    "assert isinstance(all_pairs_preds, np.ndarray)\n",
    "assert all_pairs_preds.shape == (1, 6)\n",
    "assert set(all_pairs_preds.flatten()).issubset({0, 1, 2})\n",
    "\n",
    "one_vs_all_acc = one_vs_all_model.accuracy(X1_test, Y1_test)\n",
    "assert isinstance(one_vs_all_acc, float)\n",
    "assert one_vs_all_acc == pytest.approx(0.8)\n",
    "\n",
    "all_pairs_acc = all_pairs_model.accuracy(X_multi_class, Y_multi_class)\n",
    "assert isinstance (all_pairs_acc, float)\n",
    "assert all_pairs_acc == pytest.approx(1/6, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a6362",
   "metadata": {},
   "source": [
    "Here, we run our model on a public Kaggle dataset - specifically the classical MNIST dataset for multi-class digit classification. Dataset credit: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbb5cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m np.random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m    102\u001b[39m random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     X_train, X_val, y_train, y_val = \u001b[43mpreprocess_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     X_train_val = np.concatenate((X_train, X_val))\n\u001b[32m     94\u001b[39m     Y_train_val = np.concatenate((y_train, y_val))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mpreprocess_mnist\u001b[39m\u001b[34m(train_path, test_path, total_samples, val_size)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03mPreprocesses the MNIST dataset. Subsets the data to a total of 5k examples for ease of running.\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m@params:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m \u001b[33;03m    X_train, X_val, X_test, y_train, y_val, y_test - split data\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Load both CSV files into dataframes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m test_df = pd.read_csv(test_path)\n\u001b[32m     78\u001b[39m df = pd.concat([train_df, test_df], axis=\u001b[32m0\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: load, clean, encode, scale, and split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def add_bias_and_numpy(df_X):\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame to a NumPy array, stacking on a bias column (all 1s) to the\n",
    "    left-hand side as needed for the model.\n",
    "    @params:\n",
    "        df_X: dataframe for conversion.\n",
    "    @return:\n",
    "        Numpy array of the features dataframe.\n",
    "    \"\"\"\n",
    "    arr = df_X.values.astype(float)\n",
    "    bias = np.ones((arr.shape[0], 1), dtype=float)\n",
    "    return np.hstack([bias, arr])\n",
    "\n",
    "def plot_loss(title, loss_dict):\n",
    "    \"\"\"\n",
    "    Plots loss curves given a loss dictionary mapping epochs -> total number of loss values calc. in that epoch.\n",
    "    The number of values per epoch corresponds to the number of classifiers ran during that epoch, which is equivalent\n",
    "    to the number of unique classes in our dataset.\n",
    "    @params\n",
    "        title: title for visualization plot.\n",
    "        loss_dict: dictionary holding our loss values per epoch\n",
    "    @return: nothing\n",
    "    \"\"\"\n",
    "    epochs, losses = [], []\n",
    "    for k, v in loss_dict.items():\n",
    "        epochs.append(k)\n",
    "        losses.append(sum(v) / len(v))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Loss Values\")\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.show()\n",
    "\n",
    "def visualization(RR : RegularizedLogisticRegression, X_train, y_train, X_val, y_val, X_train_val, Y_train_val, model_type):\n",
    "    \"\"\"\n",
    "    Visualizes the data by running k-fold cross validation on different lambda values for the classification task.\n",
    "    @params\n",
    "        X_train, y_train, X_val, y_val, X_train_val, Y_train_val: input data\n",
    "        title: title for visualization plots\n",
    "    @return: nothing\n",
    "    \"\"\"\n",
    "    # Plot loss curves\n",
    "    if RR.one_vs_all:\n",
    "        RR.one_vs_all_train(X_train, y_train)\n",
    "    elif RR.all_pairs:\n",
    "        RR.all_pairs_train(X_train, y_train)\n",
    "    print(f\"{model_type}: Train Accuracy: \" + str(RR.accuracy(X_train, y_train)))\n",
    "    print(f\"{model_type}: Validation Accuracy: \" + str(RR.accuracy(X_val, y_val)))\n",
    "    plot_loss(model_type + \": Epochs vs. Loss on Train Dataset\", RR.losses)\n",
    "\n",
    "    # Plot k fold errors\n",
    "    lambda_list = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "    train_errors, val_errors = RR.runTrainTestValSplit(lambda_list, X_train, y_train, X_val, y_val)\n",
    "    k_fold_errors = RR.runKFold(lambda_list, X_train_val, Y_train_val, 3)\n",
    "    print(lambda_list)\n",
    "    print(train_errors, val_errors, k_fold_errors)\n",
    "    RR.plotError(lambda_list, train_errors, val_errors, k_fold_errors)\n",
    "\n",
    "def preprocess_mnist(train_path=\"../data/mnist_train.csv\", test_path=\"../data/mnist_test.csv\", total_samples=2000, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Preprocesses the MNIST dataset. Subsets the data to a total of 5k examples for ease of running.\n",
    "    @params:\n",
    "        train_path: filepath to train data, test_path: filepath to test data\n",
    "        total_samples: total samples across all data\n",
    "        val_size: proportion alloc. for validation\n",
    "    @return:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test - split data\n",
    "    \"\"\"\n",
    "    # Load both CSV files into dataframes\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "    df = df.sample(n=total_samples, random_state=0)\n",
    "\n",
    "    # Extract labels and pixels and normalize pix values\n",
    "    y = df.iloc[:, 0].values.astype(np.int64)\n",
    "    X = df.iloc[:, 1:].values.astype(np.float32)\n",
    "    X /= 255.0\n",
    "\n",
    "    # Do the train/val split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def main():\n",
    "    X_train, X_val, y_train, y_val = preprocess_mnist()\n",
    "    X_train_val = np.concatenate((X_train, X_val))\n",
    "    Y_train_val = np.concatenate((y_train, y_val))\n",
    "\n",
    "    # Run visualizations for both types of models\n",
    "    visualization(RegularizedLogisticRegression(one_vs_all=False, all_pairs=True), X_train, y_train, X_val, y_val, X_train_val, Y_train_val, \"All Pairs\")\n",
    "    visualization(RegularizedLogisticRegression(one_vs_all=True, all_pairs=False), X_train, y_train, X_val, y_val, X_train_val, Y_train_val, \"One vs. All\")\n",
    "    \n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf921fe",
   "metadata": {},
   "source": [
    "Here, we try to reproduce sklearn results on the public MNIST dataset mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72830518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def compare_with_sklearn_one_vs_one():\n",
    "    X_train, X_val, y_train, y_val = preprocess_mnist()\n",
    "\n",
    "    my_ovo_model = RegularizedLogisticRegression(one_vs_all=True, all_pairs=False)\n",
    "    my_ovo_model.one_vs_all_train(X_train, y_train)\n",
    "    my_preds_ovo = my_ovo_model.predict(X_val)\n",
    "\n",
    "    C_value = 1.0 / (2.0 * 0.001)\n",
    "    sk_ovo_model = OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            C=C_value,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=10000,\n",
    "            fit_intercept=False  # because X already has a bias column\n",
    "        )\n",
    "    )\n",
    "    sk_ovo_model.fit(X_train, y_train)\n",
    "    sk_preds_ovo = sk_ovo_model.predict(X_val)\n",
    "    my_acc_ovo = np.mean(my_preds_ovo == y_val)\n",
    "    sk_acc_ovo = np.mean(sk_preds_ovo == y_val)\n",
    "\n",
    "    print(\"=== One-vs-One (all-pairs) comparison ===\")\n",
    "    print(\"Our OVO logistic regression accuracy:     \", my_acc_ovo)\n",
    "    print(\"Sklearn OVO logistic regression accuracy:\", sk_acc_ovo)\n",
    "    print(\"Predictions identical on this dataset?\", np.array_equal(my_preds_ovo, sk_preds_ovo))\n",
    "\n",
    "    return my_acc_ovo, sk_acc_ovo\n",
    "\n",
    "compare_with_sklearn_one_vs_one()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb39a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "import numpy as np\n",
    "\n",
    "def compare_with_sklearn_one_vs_one():\n",
    "    X_train, X_val, y_train, y_val = preprocess_mnist()\n",
    "\n",
    "    my_ovo_model = RegularizedLogisticRegression(one_vs_all=False, all_pairs=True)\n",
    "    my_ovo_model.all_pairs_train(X_train, y_train)\n",
    "    my_preds_ovo = my_ovo_model.predict(X_val)\n",
    "\n",
    "    C_value = 1.0 / (2.0 * 0.001)\n",
    "    sk_ovo_model = OneVsOneClassifier(\n",
    "        LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            C=C_value,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=10000,\n",
    "            fit_intercept=False  # because X already has a bias column\n",
    "        )\n",
    "    )\n",
    "    sk_ovo_model.fit(X_train, y_train)\n",
    "    sk_preds_ovo = sk_ovo_model.predict(X_val)\n",
    "    my_acc_ovo = np.mean(my_preds_ovo == y_val)\n",
    "    sk_acc_ovo = np.mean(sk_preds_ovo == y_val)\n",
    "\n",
    "    print(\"=== One-vs-One (all-pairs) comparison ===\")\n",
    "    print(\"Our OVO logistic regression accuracy:     \", my_acc_ovo)\n",
    "    print(\"Sklearn OVO logistic regression accuracy:\", sk_acc_ovo)\n",
    "    print(\"Predictions identical on this dataset?\", np.array_equal(my_preds_ovo, sk_preds_ovo))\n",
    "\n",
    "    return my_acc_ovo, sk_acc_ovo\n",
    "\n",
    "compare_with_sklearn_one_vs_one()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd5618-f282-4978-b5e2-d5c2b49149c8",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Khodabakhsh, Hojjat. MNIST Dataset. Kaggle. Accessed December 10, 2025. https://www.kaggle.com/datasets/hojjatk/mnist-dataset.\n",
    "\n",
    "Pedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825â€“2830.\n",
    "\n",
    "*Scikit-Learn â€˜OneVsOneClassifierâ€™ versus â€˜OneVsRestClassifierâ€™ | SKLearner.* Available at: https://sklearner.com/scikit-learn-onevsoneclassifier-vs-onevsrestclassifier/. (Accessed 10 Dec 2025)\n",
    "\n",
    "Shalev-Shwartz, S., and Ben-David, S. (2014) *Understanding Machine Learning: From Theory to Algorithms*. 1st edn. Cambridge university press.\n",
    "\n",
    "Zsom, Andras. â€œLecture 5: Logistic Regression.â€, Lecture, Brown University, September 18, 2025. \n",
    "\n",
    "Zsom, Andras. â€œLecture 6: SGD, Data Prep, and other Practicalities.â€, Lecture, Brown University, September 23, 2025. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
