{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddc7264-fe0c-4081-85fa-48cd79de1529",
   "metadata": {},
   "source": [
    "### **Multiclass Classification**\n",
    "Logistic regression is one type of classification algorithm that determines, for a given set of feature values, the most likely label from a given set of labels. Binary logistic regression refers to the case that there are exactly two labels to choose from while multiclass regression is the more general case where there could be any given number of labels. The type of multiclass classification we will be implementing is built based on binary logistic regression, so we first discuss the binary case. \n",
    "\n",
    "\n",
    "**Binary Logistic Regression**\n",
    "\n",
    "Suppose our data points have $d$ numeric features and are labeled as either $1$ or $0$. Rather than returning a label of $\\pm 1$, binary logistic regression returns the probability of a data point belonging to class $1$. The predicted class is then $+1$ if the probability of beloning to $1$ is greater than $50\\%$ and is $0$ if the probability of belonging to $1$ is less than $50\\%$. To find the probability of a data point beloning to class $1$, the following calculation is performed, \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}}$$\n",
    "where $x$ is the set of features and $w$ is a weights vector. \n",
    "\n",
    "Then if $\\langle w, x \\rangle > 0$, we will have $e^{-\\langle w, x \\rangle} < 1$, so our probability of being class $1$ is \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}} > \\frac{1}{1+1} = \\frac{1}{2}.$$\n",
    "Alternatively, if $\\langle w, x \\rangle < 0$, we will have $e^{-\\langle w, x \\rangle} > 1$, so our probability of being class $1$ is \n",
    "$$h(x) = \\frac{1}{1+e^{-\\langle w, x \\rangle}} < \\frac{1}{1+1} = \\frac{1}{2}.$$\n",
    "In summary, if $\\langle w, x \\rangle > 0$, then the predicted class will be $1$. If $\\langle w, x \\rangle < 0$, then the predicted class will be $0$. This is then how our decision boundary is defined; the decision boundary is all vectors $x$ such that $\\langle w, x \\rangle = 0$. All vectors that lie on the side to which $w$ is pointing will be designated $1$ while all those that lie on the side away from which $w$ is pointing will be designated $0$. Training this algorithm therefore means determining the optimal weight vector $w$. Optimal here means the weight vector that minimizes error, so we must both quanitfy the error and find some way to minimize it. We quantify the error with Binary Cross Entropy Loss: \n",
    "\n",
    "$$L_s(h_w) = -\\frac{1}{m}\\sum_{i=1}^m(y_i\\log h_w(x_i)+(1-y_i)\\log(1-h_w(x_i)))$$\n",
    "\n",
    "The next step is to find weights to minimize this. We will do that through a process called Stochastic Gradient Descent.\n",
    "\n",
    "\n",
    "**Stochastic Gradient Descent**\n",
    "\n",
    "For a given set of points with features $x_i$ and labels $y_i$, we can view the error as a function of the weight vector $w$. We want to find the $w$ that minimizes our function, and from calculus we know that the gradient of the function always points away from the minimum. We can therefore approximately locate the minimum by moving in the opposite direction of the gradient in a process known as gradient descent. Starting from some initial guess, each subsequent value is achieved by moving a set distance along the function in the opposite direction of the gradient. This set distance is the step size, and the choice of step size determines whether or not gradient descent can converge. If the step size is too small, the minimum may not have been reached after thousands of timesteps while if the step size is too big, a single step might overshoot the minimum and leave the algorithm jumping back and forth to each side of the minimum but never actually getting closer. Classical gradient descent involves using all of the training data to calculate the gradient of the loss explicitly at every step; this results in a very direct but very slow path to the minimum. Stochastic gradient descent (SGD) speeds up the process by only using a smaller subset of the data to approximate the gradient of the loss function. In SGD, the data is shuffled then split into evenly sized batches. One batch is used to calculate the gradient, then a step is taken, then the next batch is used and another step is taken, repeating until all of the batches have been used. If the algorithm still has not converged after going through all of the batches, the data is shuffled and then split into new batches and the procedure is repeated. Since we do not know a priori what the minimum is, we cannot determine convergence based on distance to the minimum. Convergence must therefore be approximated by checking how much the weights are changing with every step or checking how much the loss is changing.\n",
    "\n",
    "Using the representation, loss, and optimizer described so far, we have a fully defined method for binary classification. We now build upon this to create a multiclass classification model. \n",
    "\n",
    "\n",
    "**One-vs-all**\n",
    "\n",
    "One-vs-all is a method where a binary classification algorithm can be used for multiclass classification by splitting the multiclass problem into several binary problems. Assume that we have $k$ classes. To apply the one-vs-all method, we train $k$ binary logistic regression models, each of which predicts the probability of being in one of the specific $k$ classes. Once this has been done, a the final predicted class of each data point is determined by labeling it with the class that it had the highest probability of belonging to. Implementing this algorithm would look something like the steps outlined below. \n",
    "\n",
    "$\\text{inputs:}$\n",
    "\n",
    "S = the training set comprised of $m$ labeled sets of features $(\\vec{x}_i, y_i)$\n",
    "\n",
    "$A = \\text{ the binary classification algorithm in use, ex. binary logistic regression}$\n",
    "\n",
    "$\\text{For each } k \\in \\text{ the label set } \\mathcal{Y}:$\n",
    "\n",
    "$\\quad \\text{Define } $S_k$ to be the set of all examples but where the label is 1 if the example has label $k$ and 0 otherwise$\n",
    "\n",
    "$\\quad \\text{Define } h_k = A(S_k) \\text{ as a binary predictor that predicts 1 if } x \\in S_k$\n",
    "\n",
    "$\\text{Output:}$\n",
    "\n",
    "$\\quad \\text{Define the multiclass predictor by }h(\\vec{x}) = \\text{argmax}_{k \\in \\mathcal{Y}} h_i(\\vec{x})$\n",
    "\n",
    "In summary, the multiclass predictor is composed of one binary predictor for each class that simply predicts whether or not the data belongs to that class. These prections are then gathered and the final label is the class that $\\vec{x}$ had the highest probability of being in. \n",
    "\n",
    "**One-vs-one**\n",
    "\n",
    "The other multiclass classifier we will be examining is the one-vs-one classifier. Like the one-vs-all algorithm, the one-vs-one algorithm builds a multiclass predictor out of several binary predictors. For this predictor, for each pair $i,j$ of classes, construct a subset $S_{i,j}$ of the training data $S$ that contains only the examples that are in class $i$ or $j$. Then for each subset $S_{i,j}$, set the label of $\\vec{x}$ to $1$ if $\\vec{x}$ is in class $i$ and -1 if $x$ is in class $j$. A binary classifier $h_{i,j}$ is defined for each $S_{i,j}$. A binary classifier $h_{i,j}$ is defined for each $S_{i,j}$. To get the final prediction for the class of $\\vec{x}$, $\\vec{x}$ is fed into each of the binary classifiers $h_{i,j}$. The predicted class of $\\vec{x}$ is chosen to be the class that was predicted most frequently across all of the binary classifiers. Implementing this algorithm would look something like the steps outlined below. \n",
    "\n",
    "$\\text{inputs:}$\n",
    "\n",
    "$S = \\text{ the training set comprised of $m$ labeled sets of features}(\\vec{x}_i, y_i)$\n",
    "\n",
    "$A = \\text{ the binary classification algorithm in use, ex. binary logistic regression}$\n",
    "\n",
    "$\\text{For each } i,j \\in \\text{ the label set } \\mathcal{Y} \\text{ with } i < j:$\n",
    "\n",
    "$\\quad \\text{Construct } S_{i,j} \\text{ by adding each $\\vec{x}_k$ such that } y_k \\in \\{i,j\\}$\n",
    "\n",
    "$\\quad \\text{Define } h_{i,j} = A(S_{i,j}) \\text{ as a binary predictor that predicts either $i$ (+1) or $j$ (-1) for each input }$\n",
    "\n",
    "$\\text{Output:}$\n",
    "\n",
    "$\\quad \\text{Define the multiclass predictor by }h(\\vec{x}) = \\text{argmax}_{i \\in \\mathcal{Y}} \\left( \\sum_{j \\in \\mathcal{Y}} \\text{sign}(j-i)H_{i,j}(\\vec{x} \\right)$\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "Now that we have established the theoretical basis for these algorithms, we will implement each of them below along with several test cases to demonstrate that they are performing as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b63416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0476a",
   "metadata": {},
   "source": [
    "### Model\n",
    "Binary logistic regression model + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae0feee-d2c6-4d89-8d54-8b41df0542b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class RegularizedLogisticRegression(object):\n",
    "    '''\n",
    "    Implement regularized logistic regression for binary classification.\n",
    "    The weight vector w should be learned by minimizing the regularized loss\n",
    "    L(h, (x,y)) = log(1 + exp(-y <w, x>)) + lambda |w|_2^2. In other words, the objective\n",
    "    function that we are trying to minimize is the log loss for binary logistic regression \n",
    "    plus Tikhonov regularization with a coefficient of lambda.\n",
    "    '''\n",
    "    def __init__(self, batch_size = 15, one_vs_all = True, all_pairs = False):\n",
    "        self.learningRate = 0.00001\n",
    "        self.num_epochs = 10000\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.lmbda = 1\n",
    "        self.one_vs_all = one_vs_all\n",
    "        self.all_pairs = all_pairs\n",
    "\n",
    "    def one_vs_all_predict(self, X, Y):\n",
    "        '''\n",
    "        One-vs-all algorithm for multi-class classification.\n",
    "         @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            The corresponding predictions.\n",
    "        '''\n",
    "        # First, relabel the labels in the dataset. Denote the # of labels as k.\n",
    "        uq_labels = np.unique(Y)\n",
    "        \n",
    "        # This will be a (k, m) array. Later, we will take the argmax. over\n",
    "        # axis=0 to get the class w/ largest probability for that i'th point.\n",
    "        concat_logits = []\n",
    "        for label in list(uq_labels):\n",
    "\n",
    "            # Train a binary classifier for each class, collate logits\n",
    "            binarized_Y = (Y == label).astype(int)\n",
    "            self.train(X, binarized_Y)\n",
    "            logits = sigmoid_function(X @ self.weights.T).flatten()\n",
    "            \n",
    "            # Concatenate the predictions\n",
    "            concat_logits.append(logits.T)\n",
    "\n",
    "        concat_logits = np.array(concat_logits)\n",
    "\n",
    "        # At the end, take the argmax of the predictions along axis=0 as the pred label.\n",
    "        # This flattens the concat_logits array into a 1d array (1, m).\n",
    "        return np.argmax(concat_logits, axis=0)\n",
    "        \n",
    "    def all_pairs_predict(self, X, Y):\n",
    "        '''\n",
    "        All-pairs algorithm for multi-class classification.\n",
    "         @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            The corresponding predictions.\n",
    "        '''\n",
    "        # First, relabel the labels in the dataset. Denote the # of labels as k.\n",
    "        m, _ = X.shape\n",
    "        uq_labels = np.unique(Y)\n",
    "        num_labels = len(uq_labels)\n",
    "\n",
    "        # This will end up being a 3d array, of size: (num_labels x num_labels x m)\n",
    "        #                                                   i           j       k\n",
    "        # We will sum up over the j'th dimension, then argmax over the i'th dimension.\n",
    "        # This collapses both the i'th and the j'th dimension, leaving us with a (1, m) result.\n",
    "        probs_ij = [[] for _ in range(num_labels)]\n",
    "        probs_i = [0 for _ in range(num_labels)]\n",
    "\n",
    "        # Loop over all of the label combinations\n",
    "        # i'th dimension: the actual class you want to predict probabilities for.\n",
    "        # j'th dimension: all other classes you're predicting against.\n",
    "        # Summing up over the j'th dimension gets the cumulative probability for the i'th class.\n",
    "        for i in range(num_labels):\n",
    "            for j in range(num_labels):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                X_ij, Y_ij = [], []\n",
    "                \n",
    "                # Loop over all of the examples\n",
    "                for t in range(m):\n",
    "                    if Y[t] == i:\n",
    "                        X_ij.append(X[t, :])\n",
    "                        Y_ij.append(1)\n",
    "                    elif Y[t] == j:\n",
    "                        X_ij.append(X[t, :])\n",
    "                        Y_ij.append(0)\n",
    "\n",
    "                # Train binary classifier, collate logits\n",
    "                X_ij, Y_ij = np.array(X_ij), np.array(Y_ij)\n",
    "                self.train(X_ij, Y_ij)\n",
    "                probs_ij[i].append(sigmoid_function(X @ self.weights.T).T)\n",
    "\n",
    "            # Sum up over the j'th dimension. Since each j'th element is a 1d array of size (1, m)\n",
    "            # the j'th dimension is axis=0, and we want to sum up over it for all m examples.\n",
    "            probs_i[i] = np.sum(np.array(probs_ij[i]), axis=0)\n",
    "\n",
    "        # Then, once we're completely done with prediction, we can argmax over axis=0\n",
    "        # on probs_i so that we get a (1, m) array as expected.\n",
    "        return np.argmax(probs_i, axis=0)\n",
    "\n",
    "    def predict_generic(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        logits = sigmoid_function(X @ self.weights.T)\n",
    "        return (logits >= 0.5).astype(int).flatten()\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        # X (features): (m, d)\n",
    "        num_examples, num_features = X.shape\n",
    "\n",
    "        # Weights: (d, k)\n",
    "        self.weights = np.zeros((1, num_features))\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            # Shuffle the dataset\n",
    "            shuffled_indices = np.random.permutation(num_examples)\n",
    "\n",
    "            # X: (m,d) Y: (m,1) (just add an additional dim to make later matrix operations easier)\n",
    "            X, Y = X[shuffled_indices], Y[shuffled_indices].reshape(len(Y), 1)\n",
    "\n",
    "            # Iterate over ALL batches even if not evenly divisible.\n",
    "            for batch_no in range(0, num_examples, self.batch_size):\n",
    "                X_batch = X[batch_no : batch_no + self.batch_size]\n",
    "                Y_batch = Y[batch_no : batch_no + self.batch_size]\n",
    "\n",
    "                # Compute logits: (m,). Flatten from (m, 1) though since you need to broadcast w/ Y_batch which is (m,)\n",
    "                Z_batch = sigmoid_function(X_batch @ self.weights.T)\n",
    "\n",
    "                # Vectorized loss gradient matrix computation: (m, d).T x (m, 1) -> (d, 1). To be able to add self.weights, transpose.\n",
    "                gL_w = X_batch.T @ (Z_batch - Y_batch) / len(X_batch) + (2 * self.lmbda * self.weights.T)\n",
    "\n",
    "                # Note you have to transpose 'back' here \n",
    "                self.weights -= self.learningRate * gL_w.T\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        if self.all_pairs:\n",
    "            return np.mean(self.all_pairs_predict(X, Y) == Y)\n",
    "        elif self.one_vs_all:\n",
    "            return np.mean(self.one_vs_all_predict(X, Y) == Y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid option for multi-class logistic reg classification\")\n",
    "\n",
    "    def runTrainTestValSplit(self, lambda_list, X_train, Y_train, X_val, Y_val):\n",
    "        '''\n",
    "        Given the training and validation data, fit the model with training data and test it with\n",
    "        respect to each lambda. Record the training error and validation error, which are equivalent \n",
    "        to (1 - accuracy).\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X_train: a 2D Numpy array for trainig where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_train: a 1D Numpy array for training containing the corresponding labels for each example\n",
    "            X_val: a 2D Numpy array for validation where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_val: a 1D Numpy array for validation containing the corresponding labels for each example\n",
    "        @returns:\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "        '''\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        # Train model and calculate train and validation errors here for each lambda\n",
    "\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "\n",
    "            # Train, validate, log errors\n",
    "            self.train(X_train, Y_train)\n",
    "            train_errors.append(1 - self.accuracy(X_train, Y_train))\n",
    "            val_errors.append(1 - self.accuracy(X_val, Y_val))\n",
    "\n",
    "        return train_errors, val_errors\n",
    "\n",
    "    def _kFoldSplitIndices(self, dataset, k):\n",
    "        '''\n",
    "        Helper function for k-fold cross validation. Evenly split the indices of a\n",
    "        dataset into k groups.\n",
    "        For example, indices = [0, 1, 2, 3] with k = 2 may have an output\n",
    "        indices_split = [[1, 3], [2, 0]].\n",
    "        \n",
    "        Please don't change this.\n",
    "        @params:\n",
    "            dataset: a Numpy array where each row contains an example\n",
    "            k: an integer, which is the number of folds\n",
    "        @return:\n",
    "            indices_split: a list containing k groups of indices\n",
    "        '''\n",
    "        num_data = dataset.shape[0]\n",
    "        fold_size = int(num_data / k)\n",
    "        indices = np.arange(num_data)\n",
    "        np.random.shuffle(indices)\n",
    "        indices_split = np.split(indices[:fold_size*k], k)\n",
    "        return indices_split\n",
    "\n",
    "    def runKFold(self, lambda_list, X, Y, k = 3):\n",
    "        '''\n",
    "        Run k-fold cross validation on X and Y with respect to each lambda. Return all k-fold\n",
    "        errors.\n",
    "        \n",
    "        Each run of k-fold involves k iterations. For an arbitrary iteration i, the i-th fold is\n",
    "        used as testing data while the rest k-1 folds are combined as one set of training data. The k results are\n",
    "        averaged as the cross validation error.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "            k: an integer, which is the number of folds, k is 3 by default\n",
    "        @return:\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        '''\n",
    "        k_fold_errors = []\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "            # Call _kFoldSplitIndices to split indices into k groups randomly\n",
    "            indices = self._kFoldSplitIndices(X, k)\n",
    "\n",
    "            # For each iteration i = 1...k, train the model using lmbda\n",
    "            # on kâˆ’1 folds of data. Then test with the i-th fold.\n",
    "            total_fold_error = 0\n",
    "            for i in range(k):\n",
    "\n",
    "                val_indices = indices[i]\n",
    "                X_val, Y_val = X[val_indices], Y[val_indices]\n",
    "\n",
    "                # You just want to hstack the arrays in this list, since you basically have a bunch\n",
    "                # of arrays containing valid train indices and you want to combine them all now\n",
    "                train_indices = np.hstack(indices[:i] + indices[i+1:])\n",
    "\n",
    "                X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "\n",
    "                self.train(X_train, Y_train)\n",
    "                total_fold_error += 1 - self.accuracy(X_val, Y_val)\n",
    "\n",
    "            # Calculate and record the cross validation error by averaging total errors\n",
    "            k_fold_errors.append(total_fold_error / k)\n",
    "\n",
    "        return k_fold_errors\n",
    "\n",
    "    def plotError(self, lambda_list, train_errors, val_errors, k_fold_errors):\n",
    "        '''\n",
    "        Produce a plot of the cost function on the training and validation sets, and the\n",
    "        cost function of k-fold with respect to the regularization parameter lambda. Use this plot\n",
    "        to determine a valid lambda.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.semilogx(lambda_list, train_errors, label = 'training error')\n",
    "        plt.semilogx(lambda_list, val_errors, label = 'validation error')\n",
    "        plt.semilogx(lambda_list, k_fold_errors, label = 'k-fold error')\n",
    "        plt.xlabel('lambda')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37571d1b",
   "metadata": {},
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698e225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# TODO: (this is a high level overview, don't write code for this comment) - write tests testing all_pairs AND one_vs_all.\n",
    "\n",
    "\n",
    "one_vs_all_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=True, all_pairs=False)\n",
    "all_pairs_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=False, all_pairs=True)\n",
    "\n",
    "X1 = np.array([[0, 4, 1], [0, 3, 1], [5, 0, 1], [4, 1, 1], [0, 5, 1]])\n",
    "Y1 = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "X1_test = np.array([[0, 0, 1], [-5, 3, 1], [9, 0, 1], [1, 0, 1], [6, -7, 1]])\n",
    "Y1_test = np.array([0, 0, 1, 0, 1])\n",
    "X_multi_class = np.array([[1, 0, 1], [0, 1, 1], [2, 1, 1], [3, 0, 1], [0, 3, 1], [4, 0, 1]])\n",
    "Y_multi_class = np.array([0, 1, 2, 2, 1, 0])\n",
    "\n",
    "\n",
    "# train one vs. all model\n",
    "\n",
    "one_vs_all_model.train (X1, Y1)\n",
    "w1 = one_vs_all_model.weights\n",
    "assert isinstance(w1, np.ndarray)\n",
    "assert w1.shape == (1, 3)\n",
    "assert w1 == pytest.approx(np.array([[0.1266, -0.1466, -0.0124]]), abs=0.05)\n",
    "\n",
    "\n",
    "# check all pairs model\n",
    "all_pairs_model.train(X1, Y1)\n",
    "w_all_pairs = all_pairs_model.weights\n",
    "assert isinstance(w_all_pairs, np.ndarray)\n",
    "assert w_all_pairs.shape == (1, 3)\n",
    "assert w_all_pairs == pytest.approx(np.array([[0.1266, -0.1466, -0.0124]]), abs=0.05)\n",
    "\n",
    "# test model prediction for both\n",
    "one_vs_all_pred = one_vs_all_model.predict_generic(X1_test)\n",
    "assert isinstance(one_vs_all_pred, np.ndarray)\n",
    "assert one_vs_all_pred.shape == (5,)\n",
    "assert (one_vs_all_pred == np.array([0, 0, 1, 1, 1])).all()\n",
    "one_vs_all_mc_model = RegularizedLogisticRegression(batch_size=3, one_vs_all=True, all_pairs=False)\n",
    "one_vs_all_mc_model.train(X_multi_class, Y_multi_class == 0)\n",
    "one_vs_all_mc_preds = one_vs_all_mc_model.one_vs_all_predict(X_multi_class, Y_multi_class)\n",
    "assert isinstance (one_vs_all_mc_preds, np.ndarray)\n",
    "assert one_vs_all_mc_preds.shape == (6,)\n",
    "assert set(one_vs_all_mc_preds).issubset({0, 1, 2})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_pairs_model.train(X_multi_class, Y_multi_class == 0)\n",
    "all_pairs_preds = all_pairs_model.all_pairs_predict(X_multi_class, Y_multi_class)\n",
    "assert isinstance(all_pairs_preds, np.ndarray)\n",
    "assert all_pairs_preds.shape == (1, 6)\n",
    "assert set(all_pairs_preds.flatten()).issubset({0, 1, 2})\n",
    "\n",
    "one_vs_all_acc = one_vs_all_model.accuracy(X1_test, Y1_test)\n",
    "assert isinstance(one_vs_all_acc, float)\n",
    "assert one_vs_all_acc == pytest.approx(0.8)\n",
    "\n",
    "all_pairs_acc = all_pairs_model.accuracy (X_multi_class, Y_multi_class)\n",
    "assert isinstance (all_pairs_acc, float)\n",
    "assert all_pairs_acc == pytest.approx(2/3, rel=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6f0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f04c132",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a6362",
   "metadata": {},
   "source": [
    "Dataset credit: https://www.kaggle.com/datasets/rouzbeh/introds. \n",
    "\n",
    "We are running our model on a dataset for predicting a type of drug based on various other metadata e.g., blood pressure, cholesterol, sex, etc. There are 5 different classes of drug, making this a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbb5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (160, 8), X_val shape: (40, 8)\n",
      "y_train shape: (160,), y_val shape: (40,)\n",
      "Target classes: ['drugA', 'drugB', 'drugC', 'drugX', 'drugY']\n",
      "Train Accuracy: 0.66875\n",
      "Validation Accuracy: 0.725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m np.random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m    105\u001b[39m random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# TODO: This is just copied and pasted from the HW4 assignment. But we should make other visualizations.\u001b[39;00m\n\u001b[32m     96\u001b[39m lambda_list = [\u001b[32m1000\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.001\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m train_errors, val_errors = \u001b[43mRR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunTrainTestValSplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m k_fold_errors = RR.runKFold(lambda_list, X_train_val, Y_train_val, \u001b[32m3\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(lambda_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mRegularizedLogisticRegression.runTrainTestValSplit\u001b[39m\u001b[34m(self, lambda_list, X_train, Y_train, X_val, Y_val)\u001b[39m\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# Train, validate, log errors\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.train(X_train, Y_train)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     train_errors.append(\u001b[32m1\u001b[39m - \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    201\u001b[39m     val_errors.append(\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.accuracy(X_val, Y_val))\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_errors, val_errors\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mRegularizedLogisticRegression.accuracy\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mOutput the accuracy of the trained model on a given testing dataset X and labels Y.\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m@params:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m    a float number indicating accuracy (between 0 and 1)\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_pairs:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_pairs_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m == Y)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.one_vs_all:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(\u001b[38;5;28mself\u001b[39m.one_vs_all_predict(X, Y) == Y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mRegularizedLogisticRegression.all_pairs_predict\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# Train binary classifier, collate logits\u001b[39;00m\n\u001b[32m     99\u001b[39m     X_ij, Y_ij = np.array(X_ij), np.array(Y_ij)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ij\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_ij\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     probs_ij[i].append(sigmoid_function(X @ \u001b[38;5;28mself\u001b[39m.weights.T).T)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Sum up over the j'th dimension. Since each j'th element is a 1d array of size (1, m)\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# the j'th dimension is axis=0, and we want to sum up over it for all m examples.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mRegularizedLogisticRegression.train\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    153\u001b[39m gL_w = X_batch.T @ (Z_batch - Y_batch) / \u001b[38;5;28mlen\u001b[39m(X_batch) + (\u001b[32m2\u001b[39m * \u001b[38;5;28mself\u001b[39m.lmbda * \u001b[38;5;28mself\u001b[39m.weights.T)\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Note you have to transpose 'back' here \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28mself\u001b[39m.weights -= \u001b[38;5;28mself\u001b[39m.learningRate * gL_w.T\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Data preprocessing: load, clean, encode, scale, and split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def add_bias_and_numpy(df_X):\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame to a NumPy array, stacking on a bias column (all 1s) to the\n",
    "    left-hand side as needed for the model.\n",
    "    @params:\n",
    "        df_X: dataframe for conversion.\n",
    "    @return:\n",
    "        Numpy array of the features dataframe.\n",
    "    \"\"\"\n",
    "    arr = df_X.values.astype(float)\n",
    "    bias = np.ones((arr.shape[0], 1), dtype=float)\n",
    "    return np.hstack([bias, arr])\n",
    "\n",
    "# Define default constants prior to preprocessing\n",
    "DEFAULT_TEST_SIZE = 0.2\n",
    "DEFAULT_VAL_SIZE = 0.1\n",
    "DEFAULT_CSV_FILEPATH = \"../data/Drug.csv\"\n",
    "\n",
    "def preprocess_data(\n",
    "    csv_filepath=DEFAULT_CSV_FILEPATH,\n",
    "    target_col=\"Drug\",\n",
    "    test_size=DEFAULT_TEST_SIZE,\n",
    "    val_size=DEFAULT_VAL_SIZE\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocesses the CSV file for classification.\n",
    "    @params:\n",
    "      csv_filepath: path to CSV file\n",
    "      target_col: column name of the target label in the CSV\n",
    "      test_size: fraction used as final test set\n",
    "      val_size: fraction used for validation (relative to the whole dataset)\n",
    "      random_state: seed for reproducibility\n",
    "      scale: whether to standardize numeric features (fit on training set only)\n",
    "      drop_first_dummy: whether to drop the first category in OHE (avoid multicollinearity)\n",
    "      return_encoders: if True, return fitted scalers/label encoders for reuse\n",
    "    @return:\n",
    "      X_train, X_val, X_test, y_train, y_val, y_test, encoders\n",
    "      encoders is a dict containing any: {'scaler': scaler, 'label_encoder': label_enc}\n",
    "    \"\"\"\n",
    "    # Load CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "    y_raw = df[target_col].copy()\n",
    "\n",
    "    # 6) One-hot encode object/categorical columns\n",
    "    categorical_cols = [c for c in X.columns if X[c].dtype == object or X[c].dtype.name == 'category']\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Split the data\n",
    "    X_train_df, X_val_df, y_train_raw, y_val_raw = train_test_split(\n",
    "        X, y_raw, test_size=test_size, random_state=0, stratify=y_raw\n",
    "    )\n",
    "\n",
    "    # Identify numeric columns and fit scaler on X_train only to avoid leakage\n",
    "    numeric_cols = X_train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    X_train_df[numeric_cols] = scaler.fit_transform(X_train_df[numeric_cols])\n",
    "    X_val_df[numeric_cols] = scaler.transform(X_val_df[numeric_cols])\n",
    "\n",
    "    # Encode the target variable for multiclass usage\n",
    "    label_enc = LabelEncoder()\n",
    "    label_enc.fit(y_raw) # Fit on whole dataset label to ensure consistent mapping across splits\n",
    "    y_train = label_enc.transform(y_train_raw)\n",
    "    y_val = label_enc.transform(y_val_raw)\n",
    "\n",
    "    # Convert feature dfs to numpy arrays and add bias column\n",
    "    X_train_np = add_bias_and_numpy(X_train_df)\n",
    "    X_val_np = add_bias_and_numpy(X_val_df)\n",
    "\n",
    "    print(f\"X_train shape: {X_train_np.shape}, X_val shape: {X_val_np.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}\")\n",
    "    print(\"Target classes:\", list(label_enc.classes_))\n",
    "\n",
    "    return X_train_np, X_val_np, y_train, y_val\n",
    "\n",
    "def main():\n",
    "    csv_filepath = \"../data/drug.csv\"\n",
    "    X_train, X_val, y_train, y_val = preprocess_data(csv_filepath=csv_filepath, target_col='Drug')\n",
    "    X_train_val = np.concatenate((X_train, X_val))\n",
    "    Y_train_val = np.concatenate((y_train, y_val))\n",
    "\n",
    "    RR = RegularizedLogisticRegression(one_vs_all=False, all_pairs=True)\n",
    "    RR.train(X_train, y_train)\n",
    "    print('Train Accuracy: ' + str(RR.accuracy(X_train, y_train)))\n",
    "    print('Validation Accuracy: ' + str(RR.accuracy(X_val, y_val)))\n",
    "\n",
    "    # TODO: This is just copied and pasted from the HW4 assignment. But we should make other visualizations.\n",
    "    lambda_list = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "    train_errors, val_errors = RR.runTrainTestValSplit(lambda_list, X_train, y_train, X_val, y_val)\n",
    "    k_fold_errors = RR.runKFold(lambda_list, X_train_val, Y_train_val, 3)\n",
    "    print(lambda_list)\n",
    "    print(train_errors, val_errors, k_fold_errors)\n",
    "    RR.plotError(lambda_list, train_errors, val_errors, k_fold_errors)\n",
    "    \n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72830518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def compare_with_sklearn(\n",
    "    csv_filepath=\"../data/drug.csv\",  \n",
    "    target_col=\"Drug\",\n",
    "    lambda_reg=1.0\n",
    "):\n",
    "    X_train, X_val, y_train, y_val = preprocess_data(\n",
    "        csv_filepath=csv_filepath,\n",
    "        target_col=target_col\n",
    "    )\n",
    "\n",
    "    positive_class = 0\n",
    "    y_train_bin = (y_train == positive_class).astype(int)\n",
    "    y_val_bin   = (y_val == positive_class).astype(int)\n",
    "\n",
    "    my_model = RegularizedLogisticRegression(one_vs_all=False, all_pairs=False)\n",
    "    my_model.lmbda = lambda_reg\n",
    "    my_model.train(X_train, y_train_bin)\n",
    "    my_val_preds = my_model.predict_generic(X_val)\n",
    "\n",
    "    C_value = 1.0 / (2.0 * lambda_reg)\n",
    "\n",
    "    sk_model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C_value,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=10000,\n",
    "        fit_intercept=False  \n",
    ")\n",
    "    sk_model.fit(X_train, y_train_bin)\n",
    "    sk_val_preds = sk_model.predict(X_val)\n",
    "\n",
    "    my_acc = np.mean(my_val_preds == y_val_bin)\n",
    "    sk_acc = np.mean(sk_val_preds == y_val_bin)\n",
    "\n",
    "    print(f\"Using positive_class index: {positive_class}\")\n",
    "    print(\"Our logistic regression accuracy:     \", my_acc)\n",
    "    print(\"Sklearn logistic regression accuracy:\", sk_acc)\n",
    "    print(\"Predictions identical on validation set?\",\n",
    "          np.array_equal(my_val_preds, sk_val_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
